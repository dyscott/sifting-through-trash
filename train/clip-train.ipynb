{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP Fine-Tuning Demo\n",
    "\n",
    "*Based on https://github.com/huggingface/transformers/tree/main/examples/pytorch/contrastive-image-text*\n",
    "*and https://github.com/clip-italian/clip-italian*\n",
    "\n",
    "Before running, download the [TrashNet dataset](https://github.com/garythung/trashnet/raw/master/data/dataset-resized.zip) and extract it to the 'dataset' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install huggingface\n",
    "%pip install datasets\n",
    "%pip install pillow\n",
    "%pip install transformers\n",
    "%pip install scikit-learn\n",
    "%pip install transformers[torch]\n",
    "%pip install tensorboardX\n",
    "# PyTorch + CUDA should be installed manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Processing / Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dylan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPConfig, CLIPImageProcessor, CLIPModel\n",
    "\n",
    "caption_map = {\n",
    "    \"cardboard\": \"a photo of cardboard\",\n",
    "    \"glass\": \"a photo of glass\",\n",
    "    \"metal\": \"a photo of metal\",\n",
    "    \"paper\": \"a photo of paper\",\n",
    "    \"plastic\": \"a photo of plastic\",\n",
    "    \"trash\": \"a photo of trash\"\n",
    "}\n",
    "\n",
    "model_name_or_path = 'openai/clip-vit-base-patch32'\n",
    "config = CLIPConfig.from_pretrained(model_name_or_path)\n",
    "processor = CLIPProcessor.from_pretrained(model_name_or_path)\n",
    "image_processor = CLIPImageProcessor.from_pretrained(model_name_or_path)\n",
    "model = CLIPModel.from_pretrained(model_name_or_path, config=config).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2527/2527 [00:00<00:00, 84235.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "ds = load_dataset(\"imagefolder\", data_dir=\"dataset\", split=\"train\")\n",
    "ds = ds.train_test_split(test_size=0.2, seed=512)\n",
    "labels = ds['train'].features['label'].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, RandomResizedCrop, RandomHorizontalFlip, ToTensor, Normalize, Resize, CenterCrop, ConvertImageDtype, InterpolationMode, ColorJitter\n",
    "import torch\n",
    "\n",
    "# Setup image transforms\n",
    "_train_transform = Compose([\n",
    "    RandomResizedCrop(config.vision_config.image_size, interpolation=InterpolationMode.BICUBIC, antialias=None),\n",
    "    RandomHorizontalFlip(),\n",
    "    ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "    ToTensor(),\n",
    "    ConvertImageDtype(torch.float),\n",
    "    Normalize(\n",
    "        mean=image_processor.image_mean,\n",
    "        std=image_processor.image_std\n",
    "    )\n",
    "])\n",
    "\n",
    "_val_transform = Compose([\n",
    "    Resize([config.vision_config.image_size], interpolation=InterpolationMode.BICUBIC, antialias=None),\n",
    "    CenterCrop(config.vision_config.image_size),\n",
    "    ToTensor(),\n",
    "    ConvertImageDtype(torch.float),\n",
    "    Normalize(\n",
    "        mean=image_processor.image_mean,\n",
    "        std=image_processor.image_std\n",
    "    )\n",
    "])\n",
    "\n",
    "def train_transform(example_batch):\n",
    "    example_batch['pixel_values'] = [\n",
    "        _train_transform(img.convert('RGB')) for img in example_batch['image']\n",
    "    ]\n",
    "    return example_batch\n",
    "\n",
    "def val_transform(example_batch):\n",
    "    example_batch['pixel_values'] = [\n",
    "        _val_transform(img.convert('RGB')) for img in example_batch['image']\n",
    "    ]\n",
    "    return example_batch\n",
    "\n",
    "# Setup text transform (tokenization)\n",
    "max_target_length = 32\n",
    "def tokenize_captions(examples):\n",
    "    captions = [caption_map[labels[l]] for l in examples['label']]\n",
    "    text_inputs = processor.tokenizer(captions, padding=True, truncation=True, max_length=max_target_length)\n",
    "    examples['input_ids'] = text_inputs['input_ids']\n",
    "    examples['attention_mask'] = text_inputs['attention_mask']\n",
    "    return examples\n",
    "\n",
    "# Perform tokenization and image transformation\n",
    "prepared_ds = ds.map(tokenize_captions, batched=True, remove_columns=['label'])\n",
    "prepared_ds['train'].set_transform(train_transform)\n",
    "prepared_ds['test'].set_transform(val_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    input_ids = torch.tensor([example[\"input_ids\"] for example in examples], dtype=torch.long)\n",
    "    attention_mask = torch.tensor([example[\"attention_mask\"] for example in examples], dtype=torch.long)\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"return_loss\": True,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dylan\\AppData\\Local\\Temp\\ipykernel_2368\\170147518.py:20: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "# Setup evaluation model input\n",
    "eval_input = {}\n",
    "eval_input['pixel_values']  = torch.stack([example[\"pixel_values\"] for example in prepared_ds['test']]).to('cuda')\n",
    "\n",
    "eval_text = [\n",
    "    \"a photo of cardboard\",\n",
    "    \"a photo of glass\",\n",
    "    \"a photo of metal\",\n",
    "    \"a photo of paper\",\n",
    "    \"a photo of plastic\",\n",
    "    \"a photo of trash\"\n",
    "]\n",
    "eval_tokens = processor.tokenizer(eval_text, padding=True, truncation=True, max_length=max_target_length)\n",
    "eval_input['input_ids'] = torch.tensor(eval_tokens['input_ids']).to('cuda')\n",
    "eval_input['attention_mask'] = torch.tensor(eval_tokens['attention_mask']).to('cuda')\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(p):\n",
    "    with torch.no_grad():\n",
    "        output = model(**eval_input)\n",
    "    logits_per_image = output.logits_per_image\n",
    "    probs = logits_per_image.softmax(dim=1).cpu().numpy()\n",
    "    predictions = np.argmax(probs, axis=1)\n",
    "    references = ds['test']['label']\n",
    "    return metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num params: 151277313\n",
      "num trainable params: 655361\n"
     ]
    }
   ],
   "source": [
    "# Freeze text and vision model\n",
    "for param in model.text_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.vision_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(f\"num params:\", model.num_parameters())\n",
    "print(f\"num trainable params:\", model.num_parameters(only_trainable=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "import os\n",
    "\n",
    "if os.path.exists('./clip-trash'):\n",
    "    last_checkpoint = get_last_checkpoint(\"./clip-trash\")\n",
    "else:\n",
    "    last_checkpoint = None\n",
    "  \n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./clip-trash\",\n",
    "  per_device_train_batch_size=16,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=64,\n",
    "  #fp16=True,\n",
    "  save_steps=200,\n",
    "  eval_steps=200,\n",
    "  logging_steps=50,\n",
    "  learning_rate=1e-4,\n",
    "  save_total_limit=1,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  report_to='tensorboard',\n",
    "  load_best_model_at_end=True,\n",
    "  resume_from_checkpoint=last_checkpoint,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_ds[\"train\"],\n",
    "    eval_dataset=prepared_ds[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 8050/8128 [00:07<00:00, 5779.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4595, 'learning_rate': 9.596456692913387e-07, 'epoch': 63.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 8100/8128 [00:14<00:00, 454.34it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4976, 'learning_rate': 3.44488188976378e-07, 'epoch': 63.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8128/8128 [00:18<00:00, 436.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 18.5818, 'train_samples_per_second': 6960.794, 'train_steps_per_second': 437.418, 'train_loss': 0.022948608154386985, 'epoch': 64.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =       64.0\n",
      "  train_loss               =     0.0229\n",
      "  train_runtime            = 0:00:18.58\n",
      "  train_samples_per_second =   6960.794\n",
      "  train_steps_per_second   =    437.418\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./clip-trash\\\\preprocessor_config.json']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_results = trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()\n",
    "processor.save_pretrained(\"./clip-trash\")\n",
    "image_processor.save_pretrained(\"./clip-trash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:23<00:00,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =       64.0\n",
      "  eval_accuracy           =     0.9209\n",
      "  eval_loss               =     0.9143\n",
      "  eval_runtime            = 0:00:23.21\n",
      "  eval_samples_per_second =     21.792\n",
      "  eval_steps_per_second   =      2.756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate(prepared_ds['test'])\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Stage Training\n",
    "\n",
    "I am not sure yet how helpful this is, as CLIP's visual and text models are already trained on a dataset of 600 million images :/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num params: 151277313\n",
      "num trainable params: 123542529\n"
     ]
    }
   ],
   "source": [
    "# Unfreeze text and vision model\n",
    "for param in model.text_model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.vision_model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Freeze embedding layer\n",
    "for param in model.vision_model.embeddings.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.text_model.embeddings.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(f\"num params:\", model.num_parameters())\n",
    "print(f\"num trainable params:\", model.num_parameters(only_trainable=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('./clip-trash-s2'):\n",
    "    last_checkpoint = get_last_checkpoint(\"./clip-trash-s2\")\n",
    "else:\n",
    "    last_checkpoint = None\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./clip-trash-s2\",\n",
    "  per_device_train_batch_size=16,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=8,\n",
    "  #fp16=True,\n",
    "  save_steps=100,\n",
    "  eval_steps=100,\n",
    "  logging_steps=20,\n",
    "  learning_rate=1e-8,\n",
    "  save_total_limit=1,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  report_to='tensorboard',\n",
    "  load_best_model_at_end=True,\n",
    "  resume_from_checkpoint=last_checkpoint,\n",
    "  #weight_decay=1e-2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_ds[\"train\"],\n",
    "    eval_dataset=prepared_ds[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1016/1016 [00:05<00:00, 202.83it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 4.8886, 'train_samples_per_second': 3307.306, 'train_steps_per_second': 207.832, 'train_loss': 0.022579181851364497, 'epoch': 8.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        8.0\n",
      "  train_loss               =     0.0226\n",
      "  train_runtime            = 0:00:04.88\n",
      "  train_samples_per_second =   3307.306\n",
      "  train_steps_per_second   =    207.832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./clip-trash-s2\\\\preprocessor_config.json']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_results = trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()\n",
    "processor.save_pretrained(\"./clip-trash-s2\")\n",
    "image_processor.save_pretrained(\"./clip-trash-s2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:28<00:00,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        8.0\n",
      "  eval_accuracy           =     0.9209\n",
      "  eval_loss               =      0.914\n",
      "  eval_runtime            = 0:00:28.99\n",
      "  eval_samples_per_second =     17.454\n",
      "  eval_steps_per_second   =      2.208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer._load_best_model()\n",
    "metrics = trainer.evaluate(prepared_ds['test'])\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
